{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2779cad8-110a-4ca4-9570-845f071d7ec8",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3d528-ffb3-4c37-89d1-2a1909d2aefd",
   "metadata": {},
   "source": [
    "##### Region = eu-west-1\n",
    "##### Jumpstart Version = 2.0.4\n",
    "##### Meta Llama-2-13b instance type = ml.g5.12xlarge\n",
    "##### llama-index==0.8.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82853c-7d13-4240-bb19-f931e821b59f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pip install the following \n",
    "llama-index==0.8.35\n",
    "\n",
    "pypdf==3.16.4\n",
    "\n",
    "transformers==4.34.1\n",
    "\n",
    "torchvision==0.16.0\n",
    "\n",
    "langchain==0.0.317\n",
    "\n",
    "langsmith==0.0.49\n",
    "\n",
    "openai==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ffad2d-036d-4813-8a70-53c823b39057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e296db00ab4be9a4eb2de120694fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7106fca1f67a4827b049af390284d410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.143187 seconds\n",
      "      |_chunking ->  0.0648 seconds\n",
      "      |_chunking ->  0.061799 seconds\n",
      "    |_embedding ->  2.762972 seconds\n",
      "    |_embedding ->  1.892869 seconds\n",
      "    |_embedding ->  1.494365 seconds\n",
      "    |_embedding ->  1.544892 seconds\n",
      "    |_embedding ->  1.509577 seconds\n",
      "    |_embedding ->  1.540883 seconds\n",
      "**********\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_query ->  4.297448 seconds\n",
      "      |_retrieve ->  0.024519 seconds\n",
      "        |_embedding ->  0.021604 seconds\n",
      "      |_synthesize ->  4.272468 seconds\n",
      "        |_templating ->  2.9e-05 seconds\n",
      "        |_llm ->  4.266244 seconds\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Sure! Based on the context information provided, here's a summary of the document:\n",
       "\n",
       "The document is a personal essay by Paul Graham, a well-known computer programmer and investor. He describes his experiences working at Interleaf, a software company, and later co-founding Y Combinator, a successful startup accelerator. Graham reflects on what he learned during his time at Interleaf, including the importance of being the \"entry-level\" option and the dangers of prestige. He also discusses the challenges of working on Hacker News, a news aggregator for startup founders, and how it became a significant source of stress for him. The essay concludes with Graham's thoughts on the value of learning from one's mistakes and the importance of working hard.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex, download_loader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.llms.sagemaker_endpoint import SagemakerEndpoint\n",
    "\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Change INFO to DEBUG if you want more extensive logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "# S3Reader = download_loader(\"S3Reader\")\n",
    "# loader = S3Reader(bucket='llm-sql', prefix='ec2_qbr_priv/financials-llama-index/data/')\n",
    "\n",
    "loader = SimpleDirectoryReader('./data', recursive=True, exclude_hidden=True)\n",
    "\n",
    "fin_docs = loader.load_data()\n",
    "\n",
    "class ContentHandlerForTextGeneration(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": [[{\"role\": \"user\", \"content\": prompt},]],\n",
    "                                  \"parameters\" : model_kwargs\n",
    "                                  })\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0]['generation']['content']\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.1,}\n",
    "region=\"eu-west-1\"\n",
    "\n",
    "endpoint_name = \"meta-textgeneration-llama-2-13b-f-2024-04-11-10-59-12-609\"\n",
    "# endpoint_name= \"huggingface-pytorch-tgi-inference-2024-04-11-11-15-17-687\"\n",
    "content_handler = ContentHandlerForTextGeneration()\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    endpoint_kwargs={\"CustomAttributes\":\"accept_eula=true\"},\n",
    "    content_handler=content_handler)\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "storage_directory = \"index\"\n",
    "# chunk_size - It defines the size of the chunks (or nodes) that documents are broken into when they are indexed by LlamaIndex\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=600,\n",
    "                                               embed_model=\"local\",\n",
    "                                               callback_manager=callback_manager)\n",
    "\n",
    "# Build the index\n",
    "index = VectorStoreIndex.from_documents(fin_docs, service_context=service_context, show_progress=True)\n",
    "\n",
    "# Persist the index to disk\n",
    "index.storage_context.persist(persist_dir=storage_directory)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_directory)\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                                     similarity_top_k=3)\n",
    "response = query_engine.query(\"Give me a summary of the document\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b4985a-a49b-4867-bec2-5bd69deb4f42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=' Sure! Based on the context information provided, here\\'s a summary of the document:\\n\\nThe document is a personal essay by Paul Graham, a well-known computer programmer and investor. He describes his experiences working at Interleaf, a software company, and later co-founding Y Combinator, a successful startup accelerator. Graham reflects on what he learned during his time at Interleaf, including the importance of being the \"entry-level\" option and the dangers of prestige. He also discusses the challenges of working on Hacker News, a news aggregator for startup founders, and how it became a significant source of stress for him. The essay concludes with Graham\\'s thoughts on the value of learning from one\\'s mistakes and the importance of working hard.', source_nodes=[NodeWithScore(node=TextNode(id_='3ff7642d-dfcb-419e-8ba4-6cf738ebf1f0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9420c010-f5e7-4fd4-85ea-34b04c01be72', node_type=None, metadata={}, hash='2e2d9629223c077019a6dde689049344ff2293d6c52372871420119ec049f25c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1aba1a2a-567c-432d-92f4-f66883aec9e6', node_type=None, metadata={}, hash='2c0a26271dbd76f67703db6c22ee8e27453b3bf8ed11a5d8dd0258409435248f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c20a3768-cb94-4e41-b456-19fc4f8ef212', node_type=None, metadata={}, hash='142780c441f0982f81cfd4c4274dfa1414961a62f854d5ea259c994c6f45e4b4')}, hash='28c8d58f8ade80c17c06e3c630c6e003dda49a0224e92d8cafb9aa413313b4a2', text='This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn\\'t startup founders we wanted to reach. It was future startup founders. So I changed the name to Hacker News and the topic to whatever engaged one\\'s intellectual curiosity.\\n\\nHN was no doubt good for YC, but it was also by far the biggest source of stress for me. If all I\\'d had to do was select and help founders, life would have been so easy. And that implies that HN was a mistake. Surely the biggest source of stress in one\\'s work should at least be something close to the core of the work. Whereas I was like someone who was in pain while running a marathon not from the exertion of running, but because I had a blister from an ill-fitting shoe. When I was dealing with some urgent problem during YC, there was about a 60% chance it had to do with HN, and a 40% chance it had do with everything else combined. [17]\\n\\nAs well as HN, I wrote all of YC\\'s internal software in Arc. But while I continued to work a good deal in Arc, I gradually stopped working on Arc, partly because I didn\\'t have time to, and partly because it was a lot less attractive to mess around with the language now that we had all this infrastructure depending on it. So now my three projects were reduced to two: writing essays and working on YC.\\n\\nYC was different from other kinds of work I\\'ve done. Instead of deciding for myself what to work on, the problems came to me. Every 6 months there was a new batch of startups, and their problems, whatever they were, became our problems. It was very engaging work, because their problems were quite varied, and the good founders were very effective. If you were trying to learn the most you could about startups in the shortest possible time, you couldn\\'t have picked a better way to do it.\\n\\nThere were parts of the job I didn\\'t like. Disputes between cofounders, figuring out when people were lying to us, fighting with people who maltreated the startups, and so on. But I worked hard even at the parts I didn\\'t like. I was haunted by something Kevin Hale once said about companies: \"No one works harder than the boss.\" He meant it both descriptively and prescriptively, and it was the second part that scared me. I wanted YC to be good, so if how hard I worked set the upper bound on how hard everyone else worked, I\\'d better work very hard.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8165303229640857), NodeWithScore(node=TextNode(id_='e6b285f2-4331-4f3a-aef9-ee68dc846733', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ea5c51fe-6234-49c3-b086-13a2687db35a', node_type=None, metadata={}, hash='2e2d9629223c077019a6dde689049344ff2293d6c52372871420119ec049f25c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a4e2cf0e-f07f-4583-b3bc-4ef786891651', node_type=None, metadata={}, hash='2c0a26271dbd76f67703db6c22ee8e27453b3bf8ed11a5d8dd0258409435248f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1b53f528-280b-489b-bf25-305322799d19', node_type=None, metadata={}, hash='142780c441f0982f81cfd4c4274dfa1414961a62f854d5ea259c994c6f45e4b4')}, hash='28c8d58f8ade80c17c06e3c630c6e003dda49a0224e92d8cafb9aa413313b4a2', text='This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn\\'t startup founders we wanted to reach. It was future startup founders. So I changed the name to Hacker News and the topic to whatever engaged one\\'s intellectual curiosity.\\n\\nHN was no doubt good for YC, but it was also by far the biggest source of stress for me. If all I\\'d had to do was select and help founders, life would have been so easy. And that implies that HN was a mistake. Surely the biggest source of stress in one\\'s work should at least be something close to the core of the work. Whereas I was like someone who was in pain while running a marathon not from the exertion of running, but because I had a blister from an ill-fitting shoe. When I was dealing with some urgent problem during YC, there was about a 60% chance it had to do with HN, and a 40% chance it had do with everything else combined. [17]\\n\\nAs well as HN, I wrote all of YC\\'s internal software in Arc. But while I continued to work a good deal in Arc, I gradually stopped working on Arc, partly because I didn\\'t have time to, and partly because it was a lot less attractive to mess around with the language now that we had all this infrastructure depending on it. So now my three projects were reduced to two: writing essays and working on YC.\\n\\nYC was different from other kinds of work I\\'ve done. Instead of deciding for myself what to work on, the problems came to me. Every 6 months there was a new batch of startups, and their problems, whatever they were, became our problems. It was very engaging work, because their problems were quite varied, and the good founders were very effective. If you were trying to learn the most you could about startups in the shortest possible time, you couldn\\'t have picked a better way to do it.\\n\\nThere were parts of the job I didn\\'t like. Disputes between cofounders, figuring out when people were lying to us, fighting with people who maltreated the startups, and so on. But I worked hard even at the parts I didn\\'t like. I was haunted by something Kevin Hale once said about companies: \"No one works harder than the boss.\" He meant it both descriptively and prescriptively, and it was the second part that scared me. I wanted YC to be good, so if how hard I worked set the upper bound on how hard everyone else worked, I\\'d better work very hard.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8165303229640857), NodeWithScore(node=TextNode(id_='4954b032-9929-44ae-a784-27818631cb17', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ea5c51fe-6234-49c3-b086-13a2687db35a', node_type=None, metadata={}, hash='2e2d9629223c077019a6dde689049344ff2293d6c52372871420119ec049f25c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f01c4bbc-e626-4c35-9def-abfa7dd13aa5', node_type=None, metadata={}, hash='4a35772440c54722a7029602dde08746fb283adf3e7fb0fc40c14638ef14bdc0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0f305880-743b-4d9b-a09b-1536e9d131b9', node_type=None, metadata={}, hash='d56febc58aee1cad1d53f8eb1e6b15f8f85049957010f986bb55db8e975a51d7')}, hash='dda455045a9140211b37e4206442e57a9fe589c3033d196f7bfcd8be6105c46e', text='But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.\\n\\nThe good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that every hour, even when I was just sitting in a meeting. By living cheaply I not only managed to save enough to go back to RISD, but also paid off my college loans.\\n\\nI learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it\\'s better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it\\'s depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there\\'s not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it.\\n\\nBut the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it\\'s good to be the \"entry level\" option, even though that will be less prestigious, because if you\\'re not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign.\\n\\nWhen I left to go back to RISD the next fall, I arranged to do freelance work for the group that did projects for customers, and this was how I survived for the next several years.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.804420168258191)], metadata={'3ff7642d-dfcb-419e-8ba4-6cf738ebf1f0': {}, 'e6b285f2-4331-4f3a-aef9-ee68dc846733': {}, '4954b032-9929-44ae-a784-27818631cb17': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e554bf-e0dd-4292-a7a2-d257d9e79565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 3ff7642d-dfcb-419e-8ba4-6cf738ebf1f0): This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote ...\\n\\n> Source (Doc id: e6b285f2-4331-4f3a-aef9-ee68dc846733): This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote ...\\n\\n> Source (Doc id: 4954b032-9929-44ae-a784-27818631cb17): But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. I...'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9908566f-f11a-4a5b-b5e7-70af5591c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                                     similarity_top_k=3, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7b376d-f41e-4156-b50e-8f7493e660dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  0.027361 seconds\n",
      "      |_retrieve ->  0.021727 seconds\n",
      "        |_embedding ->  0.018827 seconds\n",
      "      |_synthesize ->  0.005437 seconds\n",
      "        |_templating ->  2.7e-05 seconds\n",
      "        |_llm ->  0.0 seconds\n",
      "    |_llm ->  0.0 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (wrapped_llm_predict):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py\", line 342, in _call\n",
      "    resp = json.loads(line)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/llama_index/llms/base.py\", line 277, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/llama_index/llms/langchain.py\", line 63, in complete\n",
      "    output_str = self._llm.predict(prompt, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 906, in predict\n",
      "    return self(text, stop=_stop, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 866, in __call__\n",
      "    self.generate(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 646, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 534, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 521, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 1043, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py\", line 351, in _call\n",
      "    raise ValueError(f\"Error raised by streaming inference endpoint: {e}\")\n",
      "ValueError: Error raised by streaming inference endpoint: Expecting value: line 1 column 2 (char 1)\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Give me a summary of the document\")\n",
    "response.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
