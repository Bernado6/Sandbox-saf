{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2779cad8-110a-4ca4-9570-845f071d7ec8",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3d528-ffb3-4c37-89d1-2a1909d2aefd",
   "metadata": {},
   "source": [
    "##### Region = eu-west-1\n",
    "##### Jumpstart Version = 2.0.4\n",
    "##### Meta Llama-2-13b instance type = ml.g5.12xlarge\n",
    "##### llama-index==0.8.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82853c-7d13-4240-bb19-f931e821b59f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pip install the following \n",
    "llama-index==0.8.35\n",
    "\n",
    "pypdf==3.16.4\n",
    "\n",
    "transformers==4.34.1\n",
    "\n",
    "torchvision==0.16.0\n",
    "\n",
    "langchain==0.0.317\n",
    "\n",
    "langsmith==0.0.49\n",
    "\n",
    "openai==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4ffad2d-036d-4813-8a70-53c823b39057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b8bad448de405eb751e432dd71b12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27784e7d74bb4fe798a2d5dcaa6253bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.14815 seconds\n",
      "      |_chunking ->  0.06846 seconds\n",
      "      |_chunking ->  0.066087 seconds\n",
      "    |_embedding ->  1.500552 seconds\n",
      "    |_embedding ->  1.529617 seconds\n",
      "    |_embedding ->  1.539742 seconds\n",
      "    |_embedding ->  1.528634 seconds\n",
      "    |_embedding ->  1.517231 seconds\n",
      "    |_embedding ->  1.536484 seconds\n",
      "**********\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_query ->  4.87616 seconds\n",
      "      |_retrieve ->  0.031331 seconds\n",
      "        |_embedding ->  0.027563 seconds\n",
      "      |_synthesize ->  4.843677 seconds\n",
      "        |_templating ->  3.5e-05 seconds\n",
      "        |_llm ->  4.835328 seconds\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Sure! Based on the context information provided, here's a summary of the document:\n",
       "\n",
       "The document is a personal essay by Paul Graham, a well-known computer programmer and investor. He describes his experience working at Interleaf, a software company, and later co-founding Viaweb and Y Combinator, two successful startups.\n",
       "\n",
       "Graham highlights the lessons he learned during his time at Interleaf, including the importance of being the \"entry-level\" option, the dangers of prestige, and the value of working in a small, agile team. He also discusses the challenges of working on Hacker News, a news aggregator for startup founders, which was a significant source of stress for him.\n",
       "\n",
       "Overall, the document provides insight into Graham's career and the lessons he has learned along the way, as well as his thoughts on the tech industry and startup culture.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex, download_loader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.llms.sagemaker_endpoint import SagemakerEndpoint\n",
    "\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Change INFO to DEBUG if you want more extensive logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "# S3Reader = download_loader(\"S3Reader\")\n",
    "# loader = S3Reader(bucket='llm-sql', prefix='ec2_qbr_priv/financials-llama-index/data/')\n",
    "\n",
    "loader = SimpleDirectoryReader('./data', recursive=True, exclude_hidden=True)\n",
    "\n",
    "fin_docs = loader.load_data()\n",
    "\n",
    "class ContentHandlerForTextGeneration(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": [[{\"role\": \"user\", \"content\": prompt},]],\n",
    "                                  \"parameters\" : model_kwargs\n",
    "                                  })\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0]['generation']['content']\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.1,}\n",
    "region=\"eu-west-1\"\n",
    "\n",
    "endpoint_name = \"meta-textgeneration-llama-2-13b-f-2024-04-11-10-59-12-609\"\n",
    "# endpoint_name= \"huggingface-pytorch-tgi-inference-2024-04-11-11-15-17-687\"\n",
    "content_handler = ContentHandlerForTextGeneration()\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    endpoint_kwargs={\"CustomAttributes\":\"accept_eula=true\"},\n",
    "    content_handler=content_handler)\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "storage_directory = \"index\"\n",
    "# chunk_size - It defines the size of the chunks (or nodes) that documents are broken into when they are indexed by LlamaIndex\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=600,\n",
    "                                               embed_model=\"local\",\n",
    "                                               callback_manager=callback_manager)\n",
    "\n",
    "# Build the index\n",
    "index = VectorStoreIndex.from_documents(fin_docs, service_context=service_context, show_progress=True)\n",
    "\n",
    "# Persist the index to disk\n",
    "index.storage_context.persist(persist_dir=storage_directory)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=storage_directory)\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                                     similarity_top_k=3)\n",
    "response = query_engine.query(\"Give me a summary of the document\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9908566f-f11a-4a5b-b5e7-70af5591c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context,\n",
    "                                     similarity_top_k=3, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d7b376d-f41e-4156-b50e-8f7493e660dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  0.037419 seconds\n",
      "      |_retrieve ->  0.031565 seconds\n",
      "        |_embedding ->  0.027714 seconds\n",
      "      |_synthesize ->  0.00558 seconds\n",
      "        |_templating ->  2.7e-05 seconds\n",
      "        |_llm ->  0.0 seconds\n",
      "    |_llm ->  0.0 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9 (wrapped_llm_predict):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py\", line 342, in _call\n",
      "    resp = json.loads(line)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/json/__init__.py\", line 346, in loads\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    return _default_decoder.decode(s)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/llama_index/llms/base.py\", line 277, in wrapped_llm_predict\n",
      "    f_return_val = f(_self, *args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/llama_index/llms/langchain.py\", line 63, in complete\n",
      "    output_str = self._llm.predict(prompt, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 906, in predict\n",
      "    return self(text, stop=_stop, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 866, in __call__\n",
      "    self.generate(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 646, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 534, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 521, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/base.py\", line 1043, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/llms/sagemaker_endpoint.py\", line 351, in _call\n",
      "    raise ValueError(f\"Error raised by streaming inference endpoint: {e}\")\n",
      "ValueError: Error raised by streaming inference endpoint: Expecting value: line 1 column 2 (char 1)\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Give me a summary of the document\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
